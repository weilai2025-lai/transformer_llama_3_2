# Llama 3.2 3B æ¨¡å‹æ¶æ§‹åˆ†æ

## æ¨¡å‹åŸºæœ¬è³‡è¨Š

| åƒæ•¸ | å€¼ |
|------|-----|
| model_type | llama |
| hidden_size | 3072 |
| num_hidden_layers | 28 |
| num_attention_heads | 24 |
| num_key_value_heads | 8 (GQA) |
| head_dim | 128 |
| intermediate_size | 8192 |
| vocab_size | 128256 |
| max_position_embeddings | 131072 |
| rope_theta | 500000.0 |

## æ¶æ§‹åœ–

> **æ³¨æ„**ï¼šä¸‹åœ–ä¸­çš„ "Single Decoder Layer" æœƒ**é‡è¤‡ 28 æ¬¡**ï¼ˆLayer 0 ~ Layer 27ï¼‰ï¼Œæ¯å±¤çš„è¼¸å‡ºä½œç‚ºä¸‹ä¸€å±¤çš„è¼¸å…¥ã€‚

```mermaid
graph TD
    subgraph Input["ğŸ”¹ è¼¸å…¥å±¤"]
        A[input_ids<br/>shape: batch, seq_len]
    end

    subgraph Embedding["ğŸ”¹ Embedding"]
        B[embed_tokens<br/>vocab_size: 128256<br/>hidden: 3072]
    end

    A --> B

    subgraph DecoderStack["ğŸ” Decoder Stackï¼ˆé‡è¤‡ 28 æ¬¡ï¼‰"]
        direction TB
        
        LAYER_IN[/"Layer i è¼¸å…¥<br/>[batch, seq, 3072]"/]
        
        subgraph Layer["Single Decoder Layer (i = 0, 1, ..., 27)"]
            C[input_layernorm<br/>RMSNorm]
            
            subgraph Attention["Self-Attention (GQA)"]
                D1[q_proj<br/>3072 â†’ 3072<br/>24 heads]
                D2[k_proj<br/>3072 â†’ 1024<br/>8 heads]
                D3[v_proj<br/>3072 â†’ 1024<br/>8 heads]
                D4["ğŸ”„ RoPE<br/>Rotary Position Embedding<br/>å¥—ç”¨åˆ° Q å’Œ K"]
                D5["âš¡ Attention Score<br/>Q @ K^T / âˆš128"]
                D6["Softmax â†’ Ã— V"]
                D7[o_proj<br/>3072 â†’ 3072]
            end
            
            E[post_attention_layernorm<br/>RMSNorm]
            
            subgraph MLP["MLP (SwiGLU)"]
                F1[gate_proj<br/>3072 â†’ 8192]
                F2[up_proj<br/>3072 â†’ 8192]
                F3["SiLU(gate) Ã— up"]
                F4[down_proj<br/>8192 â†’ 3072]
            end
        end
        
        LAYER_OUT[\"Layer i è¼¸å‡º<br/>[batch, seq, 3072]"\]
    end

    B --> LAYER_IN
    LAYER_IN --> C
    C --> D1 & D2 & D3
    D1 --> D4
    D2 --> D4
    D4 --> D5
    D3 --> D6
    D5 --> D6
    D6 --> D7
    D7 -->|"+ residual"| E
    E --> F1 & F2
    F1 --> F3
    F2 --> F3
    F3 --> F4
    F4 -->|"+ residual"| LAYER_OUT
    
    LAYER_OUT -.->|"i < 27: é€åˆ° Layer i+1"| LAYER_IN

    subgraph Output["ğŸ”¹ è¼¸å‡ºå±¤"]
        G[Final RMSNorm]
        H[lm_head<br/>3072 â†’ 128256]
        I[logits<br/>shape: batch, seq, vocab]
    end

    LAYER_OUT -->|"i = 27: æœ€å¾Œä¸€å±¤è¼¸å‡º"| G
    G --> H
    H --> I

    style D4 fill:#e1f5fe,stroke:#0288d1
    style D5 fill:#fff3e0,stroke:#f57c00
    style D6 fill:#fff3e0,stroke:#f57c00
    style F3 fill:#f3e5f5,stroke:#7b1fa2
    style LAYER_IN fill:#e8f5e9,stroke:#388e3c
    style LAYER_OUT fill:#e8f5e9,stroke:#388e3c
```

### è³‡æ–™æµèªªæ˜

```
è¼¸å…¥ â†’ Embedding â†’ [Layer 0] â†’ [Layer 1] â†’ ... â†’ [Layer 27] â†’ Final Norm â†’ lm_head â†’ è¼¸å‡º
                      â†‘                              â†“
                      â””â”€â”€â”€â”€â”€â”€â”€â”€ é‡è¤‡ 28 æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## é—œéµçµ„ä»¶èªªæ˜

### 1. RoPE (Rotary Position Embedding)
- ä½ç½®ï¼šåœ¨ Q, K è¨ˆç®—å¾Œã€Attention Score è¨ˆç®—å‰
- ä½œç”¨ï¼šå°‡ä½ç½®è³‡è¨Šç·¨ç¢¼åˆ° query å’Œ key ä¸­
- åƒæ•¸ï¼š`rope_theta = 500000.0`

### 2. GQA (Grouped Query Attention)
- è¨­è¨ˆï¼š24 å€‹ query heads å…±äº« 8 å€‹ key/value heads
- æ¯”ä¾‹ï¼šæ¯ 3 å€‹ Q heads å…±äº« 1 å€‹ KV head
- å„ªé»ï¼šæ¸›å°‘ KV cache è¨˜æ†¶é«”ä½¿ç”¨

### 3. Attention Score è¨ˆç®—
```
Attention(Q, K, V) = softmax(Q @ K^T / sqrt(head_dim)) @ V
```
- head_dim = 128

### 4. SwiGLU MLP
```
MLP(x) = down_proj(SiLU(gate_proj(x)) * up_proj(x))
```
- intermediate_size = 8192

## è©³ç´°å±¤ç´šçµæ§‹

```
================================================================================================================================================================
Layer (type (var_name))                                      Input Shape               Output Shape              Param #                   Mult-Adds
================================================================================================================================================================
LlamaForCausalLM (LlamaForCausalLM)                          [1, 4]                    --                        --                        --
â”œâ”€LlamaModel (model)                                         --                        --                        --                        --
â”‚    â””â”€Embedding (embed_tokens)                              [1, 4]                    [1, 4, 3072]              394,002,432               394,002,432
â”‚    â””â”€LlamaRotaryEmbedding (rotary_emb)                     [1, 4, 3072]              [1, 4, 128]               --                        --
â”‚    â””â”€ModuleList (layers)                                   --                        --                        --                        --
â”‚    â”‚    â””â”€LlamaDecoderLayer (0)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (1)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (2)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (3)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (4)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (5)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (6)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (7)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (8)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (9)                            [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (10)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (11)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (12)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (13)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (14)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (15)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (16)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (17)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (18)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (19)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (20)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (21)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (22)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (23)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (24)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (25)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (26)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â”‚    â””â”€LlamaDecoderLayer (27)                           [1, 4, 3072]              [1, 4, 3072]              --                        --
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (input_layernorm)              [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaAttention (self_attn)                  --                        [1, 4, 3072]              25,165,824                25,165,824
â”‚    â”‚    â”‚    â””â”€LlamaRMSNorm (post_attention_layernorm)     [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”‚    â”‚    â”‚    â””â”€LlamaMLP (mlp)                              [1, 4, 3072]              [1, 4, 3072]              75,497,472                75,497,472
â”‚    â””â”€LlamaRMSNorm (norm)                                   [1, 4, 3072]              [1, 4, 3072]              3,072                     3,072
â”œâ”€Linear (lm_head)                                           [1, 4, 3072]              [1, 4, 128256]            394,002,432               394,002,432
================================================================================================================================================================
Total params: 3,606,752,256
Trainable params: 3,606,752,256
Non-trainable params: 0
Total mult-adds (Units.GIGABYTES): 3.61
================================================================================================================================================================
Input size (MB): 0.00
Forward/backward pass size (MB): 17.29
Params size (MB): 7213.50
Estimated Total Size (MB): 7230.79
================================================================================================================================================================
```
