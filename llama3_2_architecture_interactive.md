# Llama 3.2 3B æ¨¡å‹æ¶æ§‹åˆ†æï¼ˆäº’å‹•ç‰ˆï¼‰

## æ¨¡å‹åŸºæœ¬è³‡è¨Š

| åƒæ•¸ | å€¼ |
|------|-----|
| model_type | llama |
| hidden_size | 3072 |
| num_hidden_layers | **28** |
| num_attention_heads | 24 |
| num_key_value_heads | 8 (GQA) |
| head_dim | 128 |
| intermediate_size | 8192 |
| vocab_size | 128256 |
| rope_theta | 500000.0 |

---

## å®Œæ•´æ¶æ§‹ï¼ˆå¯å±•é–‹æ¯ä¸€å±¤ï¼‰

### ğŸ”¹ è¼¸å…¥å±¤

```
input_ids [batch, seq_len]
    â†“
embed_tokens (128256 â†’ 3072)
    â†“
hidden_states [batch, seq_len, 3072]
```

---

### ğŸ” Decoder Stackï¼ˆ28 å±¤ Transformer Blocksï¼‰

> ğŸ’¡ **é»æ“Šæ¯ä¸€å±¤å¯ä»¥å±•é–‹è©³ç´°çµæ§‹**

<details>
<summary><b>ğŸ“¦ Layer 0</b> â€” ç¬¬ä¸€å±¤ Decoder</summary>

```
hidden_states [batch, seq, 3072]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  input_layernorm (RMSNorm, eps=1e-05)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Self-Attention (GQA)                                       â”‚
â”‚  â”œâ”€â”€ q_proj: 3072 â†’ 3072 (24 heads Ã— 128 dim)              â”‚
â”‚  â”œâ”€â”€ k_proj: 3072 â†’ 1024 (8 heads Ã— 128 dim)               â”‚
â”‚  â”œâ”€â”€ v_proj: 3072 â†’ 1024 (8 heads Ã— 128 dim)               â”‚
â”‚  â”œâ”€â”€ ğŸ”„ RoPE: å¥—ç”¨åˆ° Q, K (theta=500000)                    â”‚
â”‚  â”œâ”€â”€ âš¡ Attention: softmax(Q @ K^T / âˆš128) @ V              â”‚
â”‚  â””â”€â”€ o_proj: 3072 â†’ 3072                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (+ residual)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  post_attention_layernorm (RMSNorm)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLP (SwiGLU)                                               â”‚
â”‚  â”œâ”€â”€ gate_proj: 3072 â†’ 8192                                 â”‚
â”‚  â”œâ”€â”€ up_proj:   3072 â†’ 8192                                 â”‚
â”‚  â”œâ”€â”€ SiLU(gate) Ã— up                                        â”‚
â”‚  â””â”€â”€ down_proj: 8192 â†’ 3072                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (+ residual)
hidden_states [batch, seq, 3072] â†’ Layer 1
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 1</b></summary>

```
hidden_states [batch, seq, 3072]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  input_layernorm (RMSNorm)                                  â”‚
â”‚  â†’ Self-Attention (GQA) â†’ ğŸ”„ RoPE â†’ âš¡ Attention â†’ o_proj    â”‚
â”‚  â†’ (+ residual)                                             â”‚
â”‚  â†’ post_attention_layernorm (RMSNorm)                       â”‚
â”‚  â†’ MLP (SwiGLU): gate_proj, up_proj, SiLUÃ—, down_proj       â”‚
â”‚  â†’ (+ residual)                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
hidden_states â†’ Layer 2
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 2</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 3
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 3</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 4
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 4</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 5
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 5</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 6
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 6</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 7
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 7</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 8
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 8</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 9
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 9</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 10
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 10</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 11
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 11</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 12
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 12</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 13
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 13</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 14
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 14</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 15
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 15</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 16
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 16</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 17
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 17</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 18
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 18</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 19
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 19</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 20
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 20</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 21
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 21</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 22
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 22</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 23
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 23</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 24
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 24</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 25
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 25</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 26
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 26</b></summary>

```
[Same structure as Layer 1]
hidden_states â†’ Layer 27
```
</details>

<details>
<summary><b>ğŸ“¦ Layer 27</b> â€” æœ€å¾Œä¸€å±¤ Decoder</summary>

```
hidden_states [batch, seq, 3072]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  input_layernorm (RMSNorm, eps=1e-05)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Self-Attention (GQA)                                       â”‚
â”‚  â”œâ”€â”€ q_proj: 3072 â†’ 3072 (24 heads Ã— 128 dim)              â”‚
â”‚  â”œâ”€â”€ k_proj: 3072 â†’ 1024 (8 heads Ã— 128 dim)               â”‚
â”‚  â”œâ”€â”€ v_proj: 3072 â†’ 1024 (8 heads Ã— 128 dim)               â”‚
â”‚  â”œâ”€â”€ ğŸ”„ RoPE: å¥—ç”¨åˆ° Q, K (theta=500000)                    â”‚
â”‚  â”œâ”€â”€ âš¡ Attention: softmax(Q @ K^T / âˆš128) @ V              â”‚
â”‚  â””â”€â”€ o_proj: 3072 â†’ 3072                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (+ residual)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  post_attention_layernorm (RMSNorm)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MLP (SwiGLU)                                               â”‚
â”‚  â”œâ”€â”€ gate_proj: 3072 â†’ 8192                                 â”‚
â”‚  â”œâ”€â”€ up_proj:   3072 â†’ 8192                                 â”‚
â”‚  â”œâ”€â”€ SiLU(gate) Ã— up                                        â”‚
â”‚  â””â”€â”€ down_proj: 8192 â†’ 3072                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“ (+ residual)
hidden_states [batch, seq, 3072] â†’ è¼¸å‡ºå±¤
```
</details>

---

### ğŸ”¹ è¼¸å‡ºå±¤

```
hidden_states [batch, seq, 3072]
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Final RMSNorm (eps=1e-05)                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  lm_head (Linear: 3072 â†’ 128256)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
logits [batch, seq, 128256]
```

---

## é—œéµçµ„ä»¶èªªæ˜

### 1. RoPE (Rotary Position Embedding)
- **ä½ç½®**ï¼šåœ¨ Q, K è¨ˆç®—å¾Œã€Attention Score è¨ˆç®—å‰
- **ä½œç”¨**ï¼šå°‡ä½ç½®è³‡è¨Šç·¨ç¢¼åˆ° query å’Œ key ä¸­
- **åƒæ•¸**ï¼š`rope_theta = 500000.0`

### 2. GQA (Grouped Query Attention)
- **è¨­è¨ˆ**ï¼š24 å€‹ query heads å…±äº« 8 å€‹ key/value heads
- **æ¯”ä¾‹**ï¼šæ¯ 3 å€‹ Q heads å…±äº« 1 å€‹ KV head
- **å„ªé»**ï¼šæ¸›å°‘ KV cache è¨˜æ†¶é«”ä½¿ç”¨

### 3. Attention Score è¨ˆç®—
```
Attention(Q, K, V) = softmax(Q @ K^T / âˆšhead_dim) @ V
```
- head_dim = 128

### 4. SwiGLU MLP
```
MLP(x) = down_proj(SiLU(gate_proj(x)) Ã— up_proj(x))
```
- intermediate_size = 8192

---

## åƒæ•¸é‡çµ±è¨ˆ

| çµ„ä»¶ | æ¯å±¤åƒæ•¸é‡ | ç¸½åƒæ•¸é‡ |
|------|-----------|----------|
| embed_tokens | - | 394M |
| q_proj (Ã—28) | 9.4M | 264M |
| k_proj (Ã—28) | 3.1M | 88M |
| v_proj (Ã—28) | 3.1M | 88M |
| o_proj (Ã—28) | 9.4M | 264M |
| gate_proj (Ã—28) | 25.2M | 705M |
| up_proj (Ã—28) | 25.2M | 705M |
| down_proj (Ã—28) | 25.2M | 705M |
| lm_head | - | 394M |
| **Total** | - | **3.6B** |
